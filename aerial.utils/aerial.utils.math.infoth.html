<!DOCTYPE html PUBLIC ""
    "">
<html><head><meta charset="UTF-8" /><title>aerial.utils.math.infoth documentation</title><link rel="stylesheet" type="text/css" href="css/default.css" /><link rel="stylesheet" type="text/css" href="css/highlight.css" /><script type="text/javascript" src="js/highlight.min.js"></script><script type="text/javascript" src="js/jquery.min.js"></script><script type="text/javascript" src="js/page_effects.js"></script><script>hljs.initHighlightingOnLoad();</script></head><body><div id="header"><h2>Generated by <a href="https://github.com/weavejester/codox">Codox</a></h2><h1><a href="index.html"><span class="project-title"><span class="project-name">Aerial.utils</span> <span class="project-version">1.0.8</span></span></a></h1></div><div class="sidebar primary"><h3 class="no-link"><span class="inner">Project</span></h3><ul class="index-link"><li class="depth-1 "><a href="index.html"><div class="inner">Index</div></a></li></ul><h3 class="no-link"><span class="inner">Topics</span></h3><ul><li class="depth-1 "><a href="intro.html"><div class="inner"><span>Introduction to utils</span></div></a></li></ul><h3 class="no-link"><span class="inner">Namespaces</span></h3><ul><li class="depth-1"><div class="no-link"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>aerial</span></div></div></li><li class="depth-2"><div class="no-link"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>utils</span></div></div></li><li class="depth-3 branch"><a href="aerial.utils.coll.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>coll</span></div></a></li><li class="depth-3"><div class="no-link"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>ds</span></div></div></li><li class="depth-4 branch"><a href="aerial.utils.ds.bktrees.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>bktrees</span></div></a></li><li class="depth-4 branch"><a href="aerial.utils.ds.graphs.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>graphs</span></div></a></li><li class="depth-4"><a href="aerial.utils.ds.trees.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>trees</span></div></a></li><li class="depth-3 branch"><a href="aerial.utils.io.html"><div class="inner"><span class="tree" style="top: -114px;"><span class="top" style="height: 123px;"></span><span class="bottom"></span></span><span>io</span></div></a></li><li class="depth-3"><a href="aerial.utils.math.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>math</span></div></a></li><li class="depth-4 branch"><a href="aerial.utils.math.clustering.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>clustering</span></div></a></li><li class="depth-4 branch"><a href="aerial.utils.math.combinatorics.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>combinatorics</span></div></a></li><li class="depth-4 branch current"><a href="aerial.utils.math.infoth.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>infoth</span></div></a></li><li class="depth-4 branch"><a href="aerial.utils.math.probs-stats.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>probs-stats</span></div></a></li><li class="depth-4"><a href="aerial.utils.math.scores.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>scores</span></div></a></li><li class="depth-3 branch"><a href="aerial.utils.misc.html"><div class="inner"><span class="tree" style="top: -176px;"><span class="top" style="height: 185px;"></span><span class="bottom"></span></span><span>misc</span></div></a></li><li class="depth-3"><a href="aerial.utils.string.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>string</span></div></a></li></ul></div><div class="sidebar secondary"><h3><a href="#top"><span class="inner">Public Vars</span></a></h3><ul><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-all-grams"><div class="inner"><span>all-grams</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-bi-tri-grams"><div class="inner"><span>bi-tri-grams</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-combin-joint-entropy"><div class="inner"><span>combin-joint-entropy</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-cond-entropy"><div class="inner"><span>cond-entropy</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-conditional-mutual-information"><div class="inner"><span>conditional-mutual-information</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-CREl"><div class="inner"><span>CREl</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-dice-coeff"><div class="inner"><span>dice-coeff</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-diff-fn"><div class="inner"><span>diff-fn</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-DLX.7C.7CY"><div class="inner"><span>DLX||Y</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-DX.7C.7CY"><div class="inner"><span>DX||Y</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-entropy"><div class="inner"><span>entropy</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-expected-qdict"><div class="inner"><span>expected-qdict</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-freq-jaccard-index"><div class="inner"><span>freq-jaccard-index</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-freq-xdict-dict"><div class="inner"><span>freq-xdict-dict</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-hamming"><div class="inner"><span>hamming</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-HXY"><div class="inner"><span>HXY</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-HX.7CY"><div class="inner"><span>HX|Y</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-hybrid-dictionary"><div class="inner"><span>hybrid-dictionary</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-II"><div class="inner"><span>II</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-information-capacity"><div class="inner"><span>information-capacity</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-informativity"><div class="inner"><span>informativity</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-interaction-information"><div class="inner"><span>interaction-information</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-IXY"><div class="inner"><span>IXY</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-IXY.7CZ"><div class="inner"><span>IXY|Z</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-jaccard-dist"><div class="inner"><span>jaccard-dist</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-jaccard-index"><div class="inner"><span>jaccard-index</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-jensen-shannon"><div class="inner"><span>jensen-shannon</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-joint-entropy"><div class="inner"><span>joint-entropy</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-KLD"><div class="inner"><span>KLD</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-lambda-divergence"><div class="inner"><span>lambda-divergence</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-levenshtein"><div class="inner"><span>levenshtein</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-limit-entropy"><div class="inner"><span>limit-entropy</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-limit-informativity"><div class="inner"><span>limit-informativity</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-lod-score"><div class="inner"><span>lod-score</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-log-odds"><div class="inner"><span>log-odds</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-max-qdict-entropy"><div class="inner"><span>max-qdict-entropy</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-mutual-information"><div class="inner"><span>mutual-information</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-ngram-compare"><div class="inner"><span>ngram-compare</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-ngram-vec"><div class="inner"><span>ngram-vec</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-normed-codepoints"><div class="inner"><span>normed-codepoints</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-q-1-dict"><div class="inner"><span>q-1-dict</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-q1-xdict-dict"><div class="inner"><span>q1-xdict-dict</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-raw-lod-score"><div class="inner"><span>raw-lod-score</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-reconstruct-dict"><div class="inner"><span>reconstruct-dict</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-relative-entropy"><div class="inner"><span>relative-entropy</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-seq-joint-entropy"><div class="inner"><span>seq-joint-entropy</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-shannon-entropy"><div class="inner"><span>shannon-entropy</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-TCI"><div class="inner"><span>TCI</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-total-correlation"><div class="inner"><span>total-correlation</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-tversky-index"><div class="inner"><span>tversky-index</span></div></a></li><li class="depth-1"><a href="aerial.utils.math.infoth.html#var-variation-information"><div class="inner"><span>variation-information</span></div></a></li></ul></div><div class="namespace-docs" id="content"><h1 class="anchor" id="top">aerial.utils.math.infoth</h1><div class="doc"><pre class="plaintext">Various Information Theory functions and measures rooted in Shannon
Entropy measure and targetting a variety of sequence and string
data.</pre></div><div class="public anchor" id="var-all-grams"><h3>all-grams</h3><div class="usage"><code>(all-grams s)</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-bi-tri-grams"><h3>bi-tri-grams</h3><div class="usage"><code>(bi-tri-grams s)</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-combin-joint-entropy"><h3>combin-joint-entropy</h3><div class="usage"><code>(combin-joint-entropy coll1 coll2)</code><code>(combin-joint-entropy coll1 coll2 &amp; colls)</code></div><div class="doc"><pre class="plaintext">Given a set of collections c1, c2, c3, .. cn, return the joint
entropy: - (sum (* px1..xn (log2 px1..xn)) all-pairs-over {ci}).
Where all-pairs-over is an exhaustive combination of elements of
{ci} taken n at a time, where each n-tuple has exactly one element
from each ci (i.e., the cross product of {ci}).

Reports in bits (logfn = log2) and treats [x y] [y x] elements as
the same.
</pre></div></div><div class="public anchor" id="var-cond-entropy"><h3>cond-entropy</h3><div class="usage"><code>(cond-entropy PXY PY)</code><code>(cond-entropy combinator opts coll1 coll2)</code><code>(cond-entropy combinator opts coll1 coll2 &amp; colls)</code></div><div class="doc"><pre class="plaintext">Given the joint probability distribution PXY and the distribution
PY, return the conditional entropy of X given Y = H(X,Y) - H(Y).

Alternatively, given a set of collections c1, c2, c3, .. cn, and
combinator, a function of n variables which generates joint
occurances from {ci}, returns the multivariate conditional entropy
induced from the joint probability distributions.

OPTS is a map of options, currently sym? and logfn.  The defaults
are false and log2.  If sym? is true, treat [x y] and [y x] as
equal.  logfn can be used to provide a log of a different base.
log2, the default, reports in bits.  If no options are required,
the empty map must be passed: (cond-entropy transpose {} my-coll)

For the case of i &gt; 2, uses the recursive chain rule for
conditional entropy (bottoming out in the two collection case):

H(X1,..Xn-1|Xn) = (sum H(Xi|Xn,X1..Xi-1) (range 1 (inc n)))

</pre></div></div><div class="public anchor" id="var-conditional-mutual-information"><h3>conditional-mutual-information</h3><div class="usage"><code>(conditional-mutual-information combinator opts collx colly collz)</code></div><div class="doc"><pre class="plaintext">Conditional mutual information I(X;Y|Z).  Conditional mutual
information is the relative entropy (KLD) of the conditional
distribution (of two random variables on a third) with the product
distribution of the distributed conditionals.

Here these arise out of frequencies for the information in collz
individually, collx &amp; collz and colly &amp; collz jointly as the result
of combinator, and collx &amp; colly &amp; collz jointly as the result of
combinator as well.  Hence, combinator needs to be able to handle
the cases of 2 and 3 collections passed to it (collectively or as
variadic).

OPTS is a map of options, currently sym? and logfn.  The defaults
are false and log2.  If sym? is true, treat [x y] and [y x] as
equal.  logfn can be used to provide a log of a different base.
log2, the default, reports in bits.  If no options are required,
the empty map must be passed: (conditional-mutual-information
transpose {} my-coll)

Other interpretations/meanings: Conditional mutual information is
the mutual information of two random variables conditioned on a
third.  Or, put another way, it is the expected value of the mutual
information of two RV over the values of a third.  It measures the
amount of information shared by X &amp; Y beyond that provided by a
common "mediator".

Let XYZ be (combinator collx colly collz)
    XZ  be (combinator collx collz)
    YZ  be (combinator colly collz)

Computes

sum (* pxy|z (log2 (/ pxy|z (* px|z py|z)))) xs ys zs =

 ... (some algebra and applications of Bayes) ...

sum (* pxyz (log2 (/ (* pz pxyz) (* pxz pyz)))) xyzs xzs yzs

 ... (some more algebra and entropy identitities) ...

(+ H(X,Z) H(Y,Z) -H(X,Y,Z) -H(Z))

which is easier to work with...
</pre></div></div><div class="public anchor" id="var-CREl"><h3>CREl</h3><div class="usage"><code>(CREl l sq &amp; {:keys [limit alpha], :or {limit 15}})</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-dice-coeff"><h3>dice-coeff</h3><div class="usage"><code>(dice-coeff s1 s2)</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-diff-fn"><h3>diff-fn</h3><div class="usage"><code>(diff-fn f)</code></div><div class="doc"><pre class="plaintext">Return the function that is 1-F applied to its args: (1-(apply f
args)).  Intended for normalized distance metrics.

Ex: (let [dice-diff (diff-fn dice-coeff) ...]
      (dice-diff some-set1 some-set2))
</pre></div></div><div class="public anchor" id="var-DLX.7C.7CY"><h3>DLX||Y</h3><div class="usage"><code>(DLX||Y l Pdist Qdist)</code></div><div class="doc"><pre class="plaintext">Synonym for lambda divergence
</pre></div></div><div class="public anchor" id="var-DX.7C.7CY"><h3>DX||Y</h3><div class="usage"><code>(DX||Y &amp; args)</code></div><div class="doc"><pre class="plaintext">Synonym for relative-entropy.
args is [pd1 pd2 &amp; {:keys [logfn] :or {logfn log2}}</pre></div></div><div class="public anchor" id="var-entropy"><h3>entropy</h3><div class="usage"><code>(entropy dist &amp; {logfn :logfn, :or {logfn log2}})</code></div><div class="doc"><pre class="plaintext">Entropy calculation for the probability distribution dist.
Typically dist is a map giving the PMF of some sample space.  If it
is a string or vector, this calls shannon-entropy on dist.
</pre></div></div><div class="public anchor" id="var-expected-qdict"><h3>expected-qdict</h3><div class="usage"><code>(expected-qdict q-1 q-2 &amp; {:keys [alpha], :or {alpha ["A" "U" "G" "C"]}})</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-freq-jaccard-index"><h3>freq-jaccard-index</h3><div class="usage"><code>(freq-jaccard-index s1 s2)</code></div><div class="doc"><pre class="plaintext">
</pre></div></div><div class="public anchor" id="var-freq-xdict-dict"><h3>freq-xdict-dict</h3><div class="usage"><code>(freq-xdict-dict q sq)</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-hamming"><h3>hamming</h3><div class="usage"><code>(hamming s1 s2)</code></div><div class="doc"><pre class="plaintext">Compute hamming distance between sequences S1 and S2. If both s1
and s2 are strings, performs an optimized version</pre></div></div><div class="public anchor" id="var-HXY"><h3>HXY</h3><div class="usage"><code>(HXY &amp; args)</code></div><div class="doc"><pre class="plaintext">Synonym for joint-entropy
</pre></div></div><div class="public anchor" id="var-HX.7CY"><h3>HX|Y</h3><div class="usage"><code>(HX|Y &amp; args)</code></div><div class="doc"><pre class="plaintext">Synonym for cond-entropy
</pre></div></div><div class="public anchor" id="var-hybrid-dictionary"><h3>hybrid-dictionary</h3><div class="usage"><code>(hybrid-dictionary l sqs)</code></div><div class="doc"><pre class="plaintext">Compute the 'hybrid', aka centroid, dictionary or Feature Frequency
Profile (FFP) for sqs.  SQS is either a collection of already
computed FFPs (probability maps) of sequences, or a collection of
sequences, or a string denoting a sequence file (sto, fasta, aln,
...) giving a collection of sequences.  In the latter cases, the
sequences will have their FFPs computed based on word/feature
length L (resolution size).  In all cases the FFPs are combined,
using the minimum entropy principle, into a joint ('hybrid' /
centroid) FFP.
</pre></div></div><div class="public anchor" id="var-II"><h3>II</h3><div class="usage"><code>(II combinator opts collx colly collz &amp; colls)</code></div><div class="doc"><pre class="plaintext">Synonym for interaction-information
</pre></div></div><div class="public anchor" id="var-information-capacity"><h3>information-capacity</h3><div class="usage"><code>(information-capacity q sq &amp; {:keys [cmpfn], :or {cmpfn jensen-shannon}})</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-informativity"><h3>informativity</h3><div class="usage"><code>(informativity q sq)</code><code>(informativity q-dict)</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-interaction-information"><h3>interaction-information</h3><div class="usage"><code>(interaction-information combinator opts collx colly collz &amp; colls)</code></div><div class="doc"><pre class="plaintext">One of two forms of multivariate mutual information provided here.
 The other is "total correlation".  Interaction information
 computes the amount of information bound up in a set of variables,
 beyond that present in _any subset_ of those variables. Well, that
 seems to be the most widely held view, but there are disputes. This
 can either indicate inhibition/redundancy or facilitation/synergy
 between the variables.  It helps to look at the 3 variable case, as
 it at least lends itself to 'information' (or info venn) diagrams.

 II(X,Y,Z) = I(X,Y|Z) - I(X,Y)

 II measures the influence of Z on the information connection
 between X and Y.  Since I(X,Y|Z) &lt; I(X,Y) is possible, II can be
 negative, as well as 0 and positive.  For example if Z completely
 determines the information content between X &amp; Y, then I(X,Y|Z) -
 I(X,Y) would be 0 (as overlap contributed by X &amp; Y alone is totally
 redundant), while for X &amp; Y independent of Z, I(X,Y) could still be
 &gt; 0.  A _negative_ interaction indicates Z inhibits (accounts for,
 causes to be irrelevant/redundant) the connection between X &amp; Y.  A
 _positive_ interaction indicates Z promotes, is synergistic with,
 or enhances the connection.  The case of 0 is obvious...

 If we use the typical info theory venn diagrams, then II is
 represented as the area in all the circles. It is possible (and
 typically so done) to draw the Hxi (xi in {X,Y,Z}) circles so that
 all 3 have an overlap.  In this case the information between any
 two (the mutual information...) is partially accounted for by the
 third.  The extra overlap accounts for the negative value
 "correction" of II in this case.

 However, it is also possible to draw the circles so that any two
 overlap but all three have null intersect.  In this case, the third
 variable provides an extra connection between the other two (beyond
 their MI), something like a "mediator".  This extra connection
 accounts for the positive value "facillitation" of II in this
 case.

 It should be reasonably clear at this point that negative II is the
 much more intuitive/typical case, and that positive cases are
 surprising and typically more interesting and informative (so to
 speak...).  A positive example, would be the case of two mutually
 exclusive causes for a common effect (third variable).  In such a
 case, knowing any two completely determines the other.

 All of this can be bumped up to N variables via typical chain rule
 recurrance.

 In this implementation, the information content "measure" is
 based on the distributions arising out of the frequencies over
 coll1 .. colln _individually_, and jointly over the result of
 combinator applied collectively to all subsets of coll1 .. colln.

 OPTS is a map of options, currently sym? and logfn.  The defaults
 are false and log2.  If sym? is true, treat [x y] and [y x] as
 equal.  logfn can be used to provide a log of a different base.
 log2, the default, reports in bits.  If no options are required,
 the empty map must be passed: (interaction-information transpose {}
 my-coll)

 For collections coll1..colln and variadic combinator over any
 subset of collections, Computes:

 (sum (fn[ss] (* (expt -1 (- n |ss|))
                 (HXY combinator sym? ss)))
      (subsets all-colls))

 |ss| = cardinality/count of subset ss.  ss is a subset of
 coll1..colln.  If sym? is true, treat reversable combined items as
 equal (see HYX/joint-entropy).

 For n=3, this reduces to (- (IXY|Z combinator sym? collx colly collz)
                             (IXY combinator sym? collx colly))

 Ex:

 (interaction-information transpose {}
   "AAAGGGUUUUAAUAUUAAAAUUU"
   "AAAGGGUGGGAAUAUUCCCAUUU"
   "AAAGGGUGGGAAUAUUCCCAUUU")
 =&gt; -1.1673250256261127

 Ex:

 (interaction-information transpose {}
   "AAAGGGUUUUAAUAUUAAAAUUU"
   "AAAGGGUGGGAAUAUUCCCAUUU"
   (reverse-compliment "AAAGGGUGGGAAUAUUCCCAUUU"))
 =&gt; -0.282614135781623

 Ex: Shows synergy (positive value)

 (let [X1 [0 1] ; 7 independent binary vars
       X2 [0 1]
       X3 [0 1]
       X4 [0 1]
       X5 [0 1]
       X6 [0 1]
       X7 [0 1]
       X8 [0 1]
       Y1 [X1 X2 X3 X4 X5 X6 X7]
       Y2          [X4 X5 X6 X7]
       Y3             [X5 X6 X7]
       Y4                   [X7]
       bitval #(loop [bits (reverse %) n 0 v 0]
                 (if (empty? bits)
                   v
                   (recur (rest bits)
                          (inc n)
                          (+ v (* (first bits) (math/expt 2 n))))))
       rfn #(map bitval (apply reducem (fn[&amp; args] [args]) concat %))
       ;; Turn to (partially redundant) value sets - each Yi is the set
       ;; of all numbers for all (count Yi) bit patterns.  So, Y1, Y2,
       ;; and Y3 share all values of 3 bits, while the group with Y4
       ;; added shares just all 1 bit patterns
       Y1 (rfn Y1)
       Y2 (rfn Y2)
       Y3 (rfn Y3)
       Y4 (rfn Y4)]
   [(interaction-information concat {} Y1 Y2 Y3)
    (interaction-information concat {} Y1 Y2 Y3 Y4)])
=&gt; [-3.056592722891465   ; Intuitive, since 3 way sharing
     1.0803297840536592] ; Unintuitive, since 1 way sharing induces synergy!?
</pre></div></div><div class="public anchor" id="var-IXY"><h3>IXY</h3><div class="usage"><code>(IXY combinator opts coll1 coll2)</code></div><div class="doc"><pre class="plaintext">Synonym for mutual information
</pre></div></div><div class="public anchor" id="var-IXY.7CZ"><h3>IXY|Z</h3><div class="usage"><code>(IXY|Z combinator opts collx colly collz)</code></div><div class="doc"><pre class="plaintext">Synonym for conditional mutual information
</pre></div></div><div class="public anchor" id="var-jaccard-dist"><h3>jaccard-dist</h3><div class="usage"><code>(jaccard-dist s1 s2)</code></div><div class="doc"><pre class="plaintext">Named version of (diff-fn jaccard-index s1 s2).  This difference
function is a similarity that is a proper _distance_ metric (hence
usable in metric trees like bk-trees).</pre></div></div><div class="public anchor" id="var-jaccard-index"><h3>jaccard-index</h3><div class="usage"><code>(jaccard-index s1 s2)</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-jensen-shannon"><h3>jensen-shannon</h3><div class="usage"><code>(jensen-shannon Pdist Qdist)</code></div><div class="doc"><pre class="plaintext">Computes Jensen-Shannon Divergence of the two distributions Pdist
and Qdist.  Pdist and Qdist _must_ be over the same sample space!</pre></div></div><div class="public anchor" id="var-joint-entropy"><h3>joint-entropy</h3><div class="usage"><code>(joint-entropy combinator opts coll)</code><code>(joint-entropy combinator opts coll &amp; colls)</code></div><div class="doc"><pre class="plaintext">Given a set of collections c1, c2, c3, .. cn, and combinator, a
function of n variables which generates joint occurances from {ci},
returns the joint entropy over all the set:

-sum(* px1..xn (log2 px1..xn))

OPTS is a map of options, currently sym? and logfn.  The defaults
are false and log2.  If sym? is true, treat [x y] and [y x] as
equal.  logfn can be used to provide a log of a different base.
log2, the default, reports in bits.  If no options are required,
the empty map must be passed: (joint-entropy transpose {} my-coll)
</pre></div></div><div class="public anchor" id="var-KLD"><h3>KLD</h3><div class="usage"><code>(KLD &amp; args)</code></div><div class="doc"><pre class="plaintext">Synonym for relative-entropy.
args is [pd1 pd2 &amp; {:keys [logfn] :or {logfn log2}}</pre></div></div><div class="public anchor" id="var-lambda-divergence"><h3>lambda-divergence</h3><div class="usage"><code>(lambda-divergence lambda Pdist Qdist)</code></div><div class="doc"><pre class="plaintext">Computes a symmetrized KLD variant based on a probability
parameter, typically notated lambda, in [0..1] for each
distribution:

(+ (* lambda (DX||Y Pdist M)) (* (- 1 lambda) (DX||Y Qdist M)))

Where M = (+ (* lambda Pdist) (* (- 1 lambda) Qdist))
        = (merge-with (fn[pi qi] (+ (* lambda pi) (* (- 1 lambda) qi)))
                      Pdist Qdist)

For lambda = 1/2, this reduces to

M = 1/2 (merge-with (fn[pi qi] (+ pi qi)) P Q)

and (/ (+ (DX||Y Pdist M) (DX||Y Qdist M)) 2) = jensen shannon
</pre></div></div><div class="public anchor" id="var-levenshtein"><h3>levenshtein</h3><div class="usage"><code>(levenshtein s t)</code></div><div class="doc"><pre class="plaintext">Compute the Levenshtein (edit) distance between S and T, where S
and T are either sequences or strings.

Examples:  (levenshtein [1 2 3 4] [1 1 3]) ==&gt; 2
           (levenshtein "abcde" "bcdea")   ==&gt; 2
</pre></div></div><div class="public anchor" id="var-limit-entropy"><h3>limit-entropy</h3><div class="usage"><code>(limit-entropy q|q-dict sq|q-1dict &amp; {:keys [alpha NA], :or {alpha ["A" "U" "G" "C"], NA -1.0}})</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-limit-informativity"><h3>limit-informativity</h3><div class="usage"><code>(limit-informativity q sq)</code><code>(limit-informativity q-dict)</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-lod-score"><h3>lod-score</h3><div class="usage"><code>(lod-score qij pi pj)</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-log-odds"><h3>log-odds</h3><div class="usage"><code>(log-odds frq1 frq2)</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-max-qdict-entropy"><h3>max-qdict-entropy</h3><div class="usage"><code>(max-qdict-entropy q &amp; {:keys [alpha], :or {alpha ["A" "U" "G" "C"]}})</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-mutual-information"><h3>mutual-information</h3><div class="usage"><code>(mutual-information combinator opts coll1 coll2)</code></div><div class="doc"><pre class="plaintext">Mutual information between the content of coll1 and coll2 as
combined by combinator, a function of two variables, presumed to be
over coll1 and coll2 returning a collection of combined elements.

OPTS is a map of options, currently sym? and logfn.  The defaults
are false and log2.  If sym? is true, treat [x y] and [y x] as
equal.  logfn can be used to provide a log of a different base.
log2, the default, reports in bits.  If no options are required,
the empty map must be passed: (mutual-information transpose {} my-coll)

Mutual information is the relative entropy (KLD) of the joint
probability distribution to the product distribution.  Here the
distributions arise out of the frequencies over coll1 and coll2
individually and jointly over the result of combinator on coll1 and
coll2.

Let C be (combinator coll1 coll2).  Computes

(sum (* pxy (log2 (/ pxy (* px py)))) xs ys) =

(+ (shannon-entropy coll1) (shannon-entropy coll2) (- (joint-entropy C))) =

Hx + Hy - Hxy = I(X,Y)

(&lt;= 0.0 I(X,Y) (min [Hx Hy]))
</pre></div></div><div class="public anchor" id="var-ngram-compare"><h3>ngram-compare</h3><div class="usage"><code>(ngram-compare s1 s2 &amp; {uc? :uc?, n :n, scfn :scfn, ngfn :ngfn, :or {n 2, uc? false, scfn dice-coeff, ngfn word-letter-pairs}})</code></div><div class="doc"><pre class="plaintext">
</pre></div></div><div class="public anchor" id="var-ngram-vec"><h3>ngram-vec</h3><div class="usage"><code>(ngram-vec s &amp; {n :n, :or {n 2}})</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-normed-codepoints"><h3>normed-codepoints</h3><div class="usage"><code>(normed-codepoints s)</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-q-1-dict"><h3>q-1-dict</h3><div class="usage"><code>(q-1-dict q-xdict)</code><code>(q-1-dict q sq)</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-q1-xdict-dict"><h3>q1-xdict-dict</h3><div class="usage"><code>(q1-xdict-dict q sq &amp; {:keys [ffn], :or {ffn probs}})</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-raw-lod-score"><h3>raw-lod-score</h3><div class="usage"><code>(raw-lod-score qij pi pj &amp; {scaling :scaling, :or {scaling 1.0}})</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-reconstruct-dict"><h3>reconstruct-dict</h3><div class="usage"><code>(reconstruct-dict l sq &amp; {:keys [alpha], :or {alpha ["A" "U" "G" "C"]}})</code></div><div class="doc"><pre class="plaintext"></pre></div></div><div class="public anchor" id="var-relative-entropy"><h3>relative-entropy</h3><div class="usage"><code>(relative-entropy pdist1 pdist2 &amp; {:keys [logfn], :or {logfn log2}})</code></div><div class="doc"><pre class="plaintext">Take two distributions (that must be over the same space) and
compute the expectation of their log ratio: Let px be the PMF of
pdist1 and py be the PMF pdist2, return

(sum (fn[px py] (* px (log2 (/ px py)))) xs ys)

Here, pdist(1|2) are maps giving the probability distributions (and
implicitly the pmfs), as provided by freqs-probs, probs,
cc-freqs-probs, combins-freqs-probs, cc-combins-freqs-probs,
et. al.  Or any map where the values are the probabilities of the
occurance of the keys over some sample space.  Any summation term
where (or (= px 0) (= py 0)), is taken as 0.0.

NOTE: maps should have same keys! If this is violated it is likely
you will get a :negRE exception or worse, bogus results.  However,
as long as the maps reflect distributions _over the same sample
space_, they do not need to be a complete sampling (a key/value for
all sample space items) - missing keys will be included as 0.0
values.

Also known as Kullback-Leibler Divergence (KLD)

KLD &gt;= 0.0 in all cases.
</pre></div></div><div class="public anchor" id="var-seq-joint-entropy"><h3>seq-joint-entropy</h3><div class="usage"><code>(seq-joint-entropy s &amp; {:keys [sym? logfn], :or {sym? false, logfn log2}})</code></div><div class="doc"><pre class="plaintext">Returns the joint entropy of a sequence with itself: -sum(* pi (log
pi)), where probabilities pi are of combinations of elements of S
taken 2 at a time.  If sym?, treat [x y] and [y x] as equal.
</pre></div></div><div class="public anchor" id="var-shannon-entropy"><h3>shannon-entropy</h3><div class="usage"><code>(shannon-entropy s &amp; {logfn :logfn, :or {logfn log2}})</code></div><div class="doc"><pre class="plaintext">Returns the Shannon entropy of a sequence: -sum(* pi (log pi)),
where i ranges over the unique elements of S and pi is the
probability of i in S: (freq i s)/(count s)</pre></div></div><div class="public anchor" id="var-TCI"><h3>TCI</h3><div class="usage"><code>(TCI combinator opts coll1 coll2 &amp; colls)</code></div><div class="doc"><pre class="plaintext">Synonym for total-correlation information
</pre></div></div><div class="public anchor" id="var-total-correlation"><h3>total-correlation</h3><div class="usage"><code>(total-correlation combinator opts coll1 coll2)</code><code>(total-correlation combinator opts coll1 coll2 &amp; colls)</code></div><div class="doc"><pre class="plaintext">One of two forms of multivariate mutual information provided here.
The other is "interaction information".  Total correlation
computes what is effectively the _total redundancy_ of the
information in the provided content - here the information in coll1
.. colln.  As such it can give somewhat unexpected answers in
certain situations.

Information content "measure" is based on the distributions
arising out of the frequencies over coll1 .. colln _individually_,
and jointly over the result of combinator applied to coll1 .. colln
collectively.

OPTS is a map of options, currently sym? and logfn.  The defaults
are false and log2.  If sym? is true, treat [x y] and [y x] as
equal.  logfn can be used to provide a log of a different base.
log2, the default, reports in bits.  If no options are required,
the empty map must be passed: (total-correlation transpose {}
my-coll)

NOTE: the "degenerate" case of only two colls, is simply mutual
information.

Let C be (combinator coll1 coll2 .. colln), so xi1..xin in C is an
element in the joint sample space, and xi in colli is an element in
a "marginal" space . Computes

sum (* px1..xn (log2 (/ px1..xn (* px1 px2 .. pxn)))) x1s x2s .. xns =

   Hx1 + Hx2 + .. + Hxn - Hx1x2..xn

(&lt;= 0.0
    TC(X1,..,Xn)
    (min|i (sum Hx1 .. Hxi Hxi+2 .. Hxn, i = 0..n-1, Hx0=Hxn+1=0)))

Ex:

(shannon-entropy "AAAUUUGGGGCCCUUUAAA")
=&gt; 1.9440097497163569

(total-correlation transpose {}
  "AAAUUUGGGGCCCUUUAAA" "AAAUUUGGGGCCCUUUAAA")
=&gt; 1.9440097497163569 ; not surprising

(total-correlation transpose {}
  "AAAUUUGGGGCCCUUUAAA" "AAAUUUGGGGCCCUUUAAA"
  "AAAUUUGGGGCCCUUUAAA" "AAAUUUGGGGCCCUUUAAA")
=&gt; 5.832029249149071 ; possibly surprising if not noting tripled redundancy
</pre></div></div><div class="public anchor" id="var-tversky-index"><h3>tversky-index</h3><div class="usage"><code>(tversky-index s1 s2 alpha beta)</code></div><div class="doc"><pre class="plaintext">Tversky index of two sets S1 and S2.  A generalized NON metric
similarity 'measure'.  Generalization is through the ALPHA and BETA
coefficients:

TI(S1,S2) = (/ |S1^S2| (+ |S1^S2| (* ALPHA |S1-S2|) (* BETA |S2-S1|)))

For example, with alpha=beta=1,  TI is jaccard-index
             with alpha=beta=1/2 TI is dice-coeff
</pre></div></div><div class="public anchor" id="var-variation-information"><h3>variation-information</h3><div class="usage"><code>(variation-information combinator opts coll1 coll2)</code></div><div class="doc"><pre class="plaintext">
</pre></div></div></div></body></html>